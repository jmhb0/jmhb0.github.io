<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FZ43FV0KSZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-FZ43FV0KSZ');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>James Burgess</title>

    <meta name="author" content="James Burgess">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- <link rel="icon" href="./images/favicon.png"> -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  James Burgess
                </p>
                <p>I am a Stanford PhD student working on computer vision and machine learning. I'm fortunate to be advised by <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a> and to be supported by the <a href="https://www.quadfellowship.org/">Quad Fellowship</a>.
                </p>
                <p>
                  My methods work focuses on vision-language models, evaluation, and diffusion models. I also develop multimodal large language models for biology research.
                </p>
                <p style="text-align:center">
                  <!-- <a href="CV">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=R6GmdPIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jmhb0">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jmhb0/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jmhb/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/james_burgess_headshot.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/james_burgess_headshot_circle.png" class="hoverZoomLink" alt="james burgess headshot"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  Preamble
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <!-- Paper: Video Action Differencing  -->
    <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig1_left-viddiff.jpg'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="assets/video-action-differencing.pdf">
            <span class="papertitle">Video Action Differencing</span>
          </a>
          <br>
          <strong>James Burgess</strong>,
          <a href="https://wxh1996.github.io/">Xiaohan Wang</a>,
          <a href="https://cs.stanford.edu/~yuhuiz/">Yuhui Zhang</a>,
          <a href="https://www.linkedin.com/in/anita-rau-0aa196124/">Anita Rau</a>,
          <a href="https://www.linkedin.com/in/ale9806/">Alejandro Lozano</a>,
          <a href="https://www.lisabdunlap.com/">Lisa Dunlap</a>,
          <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
          <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>
          <br>
          <em>In review</em>
          <br>
          <a href="assets/video-action-differencing.pdf">pdf</a> / 
          <a href="https://huggingface.co/datasets/viddiff/VidDiffBench">benchmark</a> 
          <p></p>
          We propose Video Action Differencing, a new task for identifying subtle differences between videos of the same action, with a comprehensive benchmark, and an agent-based method.
        </td>
      </tr>

    <!-- Paper: MicroVQA  -->
    <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig_microvqa.png'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="assets/microvqa.pdf">
            <span class="papertitle">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</span>
          </a>
          <br>
          <strong>James Burgess</strong>*,
          <a href="https://www.linkedin.com/in/jeff-nirschl-56700918/">Jeffrey J Nirschl</a>*,
          <a href="https://laubravo.github.io/">Laura Bravo-S√°nchez</a>*,
          <a href="">Alejandro Lozano</a>,
          <a href="">Sanket Rajan Gupte</a>,
          <a href="">Jesus G. Galaz-Montoya</a>,
          <a href="">Yuhui Zhang</a>,
          <a href="">Yuchang Su</a>,
          <a href="">Disha Bhowmik</a>,
          <a href="">Zachary Coman</a>,
          <a href="">Sarina M. Hasan</a>,
          <a href="">Alexandra Johannesson</a>,
          <a href="">William D. Leineweber</a>,
          <a href="">Malvika G Nair</a>,
          <a href="">Ridhi Yarlagadda</a>,
          <a href="">Connor Zuraski</a>,
          <a href="">Wah Chiu</a>,
          <a href="">Sarah Cohen</a>,
          <a href="">Jan N. Hansen</a>,
          <a href="">Manuel D Leonetti</a>,
          <a href="">Chad Liu</a>,
          <a href="">Emma Lundberg</a>,
          <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>
          <br>
          <em>In review</em>
          <br>
          <a href="assets/microvqa.pdf">pdf</a> / 
          <a href="https://huggingface.co/datasets/microvqa/microvqa">benchmark</a> 
          <p></p>
          A benchmark for research-level reasoning in biological microscopy, with a method for making multiple-choice VQA more challenging.
        </td>
      </tr>
       
    <!-- Paper: uBench -->
      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig_ubench.jpg'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ale9806.github.io/uBench-website/">
            <span class="papertitle">Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding</span>
          </a>
          <br>
          <a href="https://www.linkedin.com/in/ale9806/">Alejandro Lozano</a>*,
          <a href="https://www.linkedin.com/in/jeff-nirschl-56700918/">Jeffrey Nirschl</a>*,
          <strong>James Burgess</strong>,
          <a href="https://www.linkedin.com/in/sanket-gupte-9b753b9a/">Sanket Rajan Gupte</a>,
          <a href="https://cs.stanford.edu/~yuhuiz/">Yuhui Zhang</a>,
          <a href="https://www.linkedin.com/in/alyssa-unell-a8a9b81a9/">Alyssa Unell</a>,
          <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>
          <br>
          <em>NeurIPS Datasets & Benchmarks 2024</em>
          <br>
          <a href="https://ale9806.github.io/uBench-website/">project page</a> / 
          <a href="">arXiv</a> /
          <a href="https://github.com/yeung-lab/u-Bench">code</a> 
          <p></p>
          <p>
            A Vision-Language Benchmark for Microscopy Understanding.
          </p>
        </td>
      </tr>
          
    <!-- Paper: Global Organelle Profiling  -->
      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig_organelle_profiling.jpg'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.biorxiv.org/content/10.1101/2023.12.18.572249v1">
            <span class="papertitle">Global organelle profiling reveals subcellular localization and remodeling at proteome scale</span>
          </a>
          <br>
          <a href="https://www.maxperutzlabs.ac.at/research/research-groups/hein">Hein et. al.</a> (including <strong>James Burgess</strong>)
          <br>
          <em>Cell 2024</em>
          <br>
          <a href="https://www.biorxiv.org/content/10.1101/2023.12.18.572249v1">bioRxiv</a>
          /
          <a href="https://github.com/jmhb0/cytoself">code</a>
          <p></p>
          <p>
            A proteomics map of human subcellular architecture, led by the Chan-Zuckerberg Biohub.
          </p>
        </td>
      </tr>

      <!-- Paper: ViewNeTI  -->
      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig_viewneti.png'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://jmhb0.github.io/view_neti/">
            <span class="papertitle">Viewpoint Textual Inversion: Discovering Scene Representations and 3D View Control in 2D Diffusion Models</span>
          </a>
          <br>
          <strong>James Burgess</strong>,
          <a href="https://wangkua1.github.io/">Kuan-Chieh Wang</a>,
          <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>
          <br>
          <em>ECCV 2024</em>
          <br>
          Outstanding Paper Award at the ECCV Workshop "Emergent Visual Abilities and Limits of Foundation Models"
          <br>
          <a href="https://jmhb0.github.io/view_neti/">project page</a>
          /
          <a href="https://arxiv.org/abs/2309.07986">arXiv</a>
          /
          <a href="https://github.com/jmhb0/view_neti">code</a>
          <p></p>
          <p>
            We show that 2D diffusion models like StableDiffusion have 3D control in their text input space which we call '3D view tokens'.
          </p>
        </td>
      </tr>


      <!-- Paper: O2vae  -->
      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/fig_o2vae.png'   width="200">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.nature.com/articles/s41467-024-45362-4">
            <span class="papertitle">Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles</span>
          </a>
          <br>
          <strong>James Burgess</strong>,
          <a href="https://www.linkedin.com/in/jeff-nirschl-56700918/">Jeffrey J. Nirschl</a>,
          <a href="https://twitter.com/MC_Zanellati">Maria-Clara Zanellati</a>,
          <a href="https://www.linkedin.com/in/ale9806/">Alejandro Lozano</a>,
          <a href="https://www.med.unc.edu/cellbiophysio/directory/sarah-cohen-phd/">Sarah Cohen</a>,
          <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>
          <br>
          <em>Nature Communications 2024</em>
          <br>
          <a href="https://www.nature.com/articles/s41467-024-45362-4">paper</a> 
          /
          <a href="https://github.com/jmhb0/o2vae">code</a>
          <p></p>
          <p>
          Unsupervised shape representations of cells and organelles are erroneously sensitive to image orientation, which we mitigate with equivariant convolutional network encoders in our method, O2VAE.
          </p>
        </td>
      </tr>
      
            
      <!-- Footer attribution for  the website template-->
      </tbody>
        </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:left;font-size:small;">
                  I stole this website template from <a href="https://jonbarron.info/">Jon Barron</a> who published his source code <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody>
        </table>



        </td>
      </tr>
    </table>
  </body>
</html>
