<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FZ43FV0KSZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      
      gtag('config', 'G-FZ43FV0KSZ');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>James Burgess</title>
    <meta name="author" content="James Burgess">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="./images/favicon.jpg">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>
  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      James Burgess
                    </p>
                    <p>I am a Stanford PhD student working on computer vision and machine learning. I'm fortunate to be advised by <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a> and to be supported by the <a href="https://www.quadfellowship.org/">Quad Fellowship</a>.
                    </p>
                    <p>
                      My methods work focuses on vision-language models, agent-based systems, and evaluation. I also develop multimodal large language models for biology research.
                    </p>
                    <p style="text-align:center">
                      <!-- <a href="CV">CV</a> &nbsp;/&nbsp; -->
                      <a href="https://scholar.google.com/citations?user=R6GmdPIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://twitter.com/jmhb0">Twitter</a> &nbsp;/&nbsp;
                      <a href="https://github.com/jmhb0/">Github</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/jmhb/">LinkedIn</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:30%;max-width:30%">
                    <a href="images/james_burgess_headshot.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/james_burgess_headshot_circle.png" class="hoverZoomLink" alt="james burgess headshot"></a>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <!-- <p>
                      Preamble
                      </p> -->
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- Paper: MicroVQA  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_microvqa.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://jmhb0.github.io/microvqa/">
                    <span class="papertitle">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</span>
                    </a>
                    <br>
                    <strong>James Burgess</strong>*,
                    Jeffrey J Nirschl*,
                    Laura Bravo-S√°nchez*,
                    Alejandro Lozano,
                    Sanket Rajan Gupte,
                    Jesus G. Galaz-Montoya,
                    Yuhui Zhang,
                    Yuchang Su,
                    Disha Bhowmik,
                    Zachary Coman,
                    Sarina M. Hasan,
                    Alexandra Johannesson,
                    William D. Leineweber,
                    Malvika G Nair,
                    Ridhi Yarlagadda,
                    Connor Zuraski,
                    Wah Chiu,
                    Sarah Cohen,
                    Jan N. Hansen,
                    Manuel D Leonetti,
                    Chad Liu,
                    Emma Lundberg,
                    Serena Yeung-Levy
                    <br>
                    <em>CVPR 2025</em>
                    <br>
                    <a href="https://jmhb0.github.io/microvqa/">project page & blog </a> /
                    <a href="">arxiv</a> / 
                    <a href="https://huggingface.co/datasets/microvqa/microvqa">benchmark</a> /
                    <a href="https://github.com/jmhb0/microvqa">code</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    MicroVQA is an expert-curated benchmark for research-level reasoning in biological microscopy. We also propose a method called RefineBot for removing language shortcuts from multiple-choice VQA.
                  </td>
                </tr>

                <!-- Paper: Biomedica  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig-biomedica.jpeg' width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://minwoosun.github.io/biomedica-website/">
                    <span class="papertitle">BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature</span>
                    </a>
                    <br>
                    Alejandro Lozano*,
                    Min Woo Sun*,
                    <strong>James Burgess</strong>*,
                    Liangyu Chen,
                    Jeffrey J. Nirschl,
                    Jeffrey Gu,
                    Ivan Lopez,
                    Josiah Aklilu,
                    Anita Rau,
                    Austin Wolfgana Katzer,
                    Collin Chiu,
                    Xiaohan Wang,
                    Alfred Seunghoon Song,
                    Robert Tibshirani,
                    Serena Yeung-Levy
                    <br>
                    <em>CVPR 2025</em>
                    <br>
                    <a href="https://minwoosun.github.io/biomedica-website/">project page</a> /
                    <a href="https://arxiv.org/abs/2501.07171">arxiv</a> / 
                    <a href="https://github.com/minwoosun/biomedica-etl">code</a> / 
                    <a href="https://huggingface.co/BIOMEDICA">data</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    The BIOMEDICA dataset has 6 million scientific articles and 24 million image-text pairs for training vision-language models in biomedicine. We use it to train state-of-the-art embedding models for biomedical images.
                  </td>
                </tr>


                <!-- Paper: Video Action Differencing  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig1_left-viddiff.jpg'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://jmhb0.github.io/viddiff/">
                    <span class="papertitle">Video Action Differencing</span>
                    </a>
                    <br>
                    <strong>James Burgess</strong>,
                    Xiaohan Wang,
                    Yuhui Zhang,
                    Anita Rau,
                    Alejandro Lozano,
                    Lisa Dunlap,
                    Trevor Darrell,
                    Serena Yeung-Levy
                    <br>
                    <em>ICLR 2025</em>
                    <br>
                    <a href="https://jmhb0.github.io/viddiff/">project page & blog</a> / 
                    <a href="https://arxiv.org/abs/2503.07860">paper</a> / 
                    <a href="https://huggingface.co/datasets/jmhb/VidDiffBench">benchmark</a> /
                    <a href="https://github.com/jmhb0/viddiff">code</a> 
                    <p></p>
                    We propose Video Action Differencing (VidDiff), a new task for detecting subtle variations in how actions are performed between two videos. We release a benchmark spaning diverse skilled actions, and a baseline method that is a simple agentic workflow.
                  </td>
                </tr>


                <!-- Paper: ViewNeTI  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_viewneti.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://jmhb0.github.io/view_neti/">
                    <span class="papertitle">Viewpoint Textual Inversion: Discovering Scene Representations and 3D View Control in 2D Diffusion Models</span>
                    </a>
                    <br>
                    <strong>James Burgess</strong>,
                    Kuan-Chieh Wang,
                    Serena Yeung-Levy
                    <br>
                    <em>ECCV 2024</em>
                    <br>
                    Outstanding Paper Award at the ECCV Workshop "Emergent Visual Abilities and Limits of Foundation Models"
                    <br>
                    <a href="https://jmhb0.github.io/view_neti/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2309.07986">arXiv</a>
                    /
                    <a href="https://github.com/jmhb0/view_neti">code</a>
                    <p></p>
                    <p>
                      We show that 2D diffusion models like StableDiffusion have 3D control in their text input space which we call '3D view tokens'.
                    </p>
                  </td>
                </tr>
                
                <!-- Paper: MicroBench -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_ubench.jpg'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ale9806.github.io/uBench-website/">
                    <span class="papertitle">Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding</span>
                    </a>
                    <br>
                    Alejandro Lozano</a>*,
                    Jeffrey Nirschl</a>*,
                    <strong>James Burgess</strong>,
                    Sanket Rajan Gupte</a>,
                    Yuhui Zhang</a>,
                    Alyssa Unell</a>,
                    Serena Yeung-Levy</a>
                    <br>
                    <em>NeurIPS Datasets & Benchmarks 2024</em>
                    <br>
                    <a href="https://ale9806.github.io/uBench-website/">project page</a> / 
                    <a href="">arXiv</a> /
                    <a href="https://github.com/yeung-lab/u-Bench">code</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    <p>
                      A Vision-Language Benchmark for Microscopy Understanding.
                    </p>
                  </td>
                </tr>

                <!-- Paper: O2vae  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_o2vae.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">
                    <span class="papertitle">Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles</span>
                    </a>
                    <br>
                    <strong>James Burgess</strong>,
                    Jeffrey J. Nirschl,
                    Maria-Clara Zanellati,
                    Alejandro Lozano,
                    Sarah Cohen,
                    Serena Yeung-Levy
                    <br>
                    <em>Nature Communications 2024</em>
                    <br>
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">paper</a> 
                    /
                    <a href="https://github.com/jmhb0/o2vae">code</a>
                    <p></p>
                    <p>
                      Unsupervised shape representations of cells and organelles are erroneously sensitive to image orientation, which we mitigate with equivariant convolutional network encoders in our method, O2VAE.
                    </p>
                  </td>
                </tr>

                <!-- Paper: Global Organelle Profiling  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_organelle_profiling.jpg'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01344-8">
                    <span class="papertitle">Global organelle profiling reveals subcellular localization and remodeling at proteome scale</span>
                    </a>
                    <br>
                    Hein et. al. (including <strong>James Burgess</strong>)
                    <br>
                    <em>Cell 2024</em>
                    <br>
                    <a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01344-8">bioRxiv</a>
                    /
                    <a href="https://github.com/jmhb0/cytoself">code</a>
                    <p></p>
                    <p>
                      A proteomics map of human subcellular architecture, led by the Chan-Zuckerberg Biohub.
                    </p>
                  </td>
                </tr>
                


              <!-- Footer attribution for  the website template-->
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Teaching</h2>
                    <br>
                    Lecturer and teaching assistant, <a href="https://web.stanford.edu/class/biods276/">CS286/BIODS276</a> <em>Advanced Topics in Computer Vision and Biomedicine</em>, Stanford 2024.
                    <br><br>
                    Teaching assistant, <a href="https://biods220.stanford.edu/">CS271/BIODS220</a>, <em>Artificial Intelligence in Healthcare</em>, Stanford 2022.
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:left;font-size:small;">
                      I stole this website template from <a href="https://jonbarron.info/">Jon Barron</a> who published his source code <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
    </table>
  </body>
</html>
